1. What is Docker?

Docker is an open-source containerization platform that allows you to package an application along with its dependencies 
(libraries, config, runtime) into a lightweight, portable container.
These containers can run consistently across different environments (developer laptop, test, production, cloud).


2. Why are you using Docker?

I use Docker mainly because:
Consistency: It eliminates the “works on my machine” problem. The same container runs everywhere.
Lightweight: Containers share the host OS kernel, so they are faster and use fewer resources compared to virtual machines.
Faster deployment: Applications start in seconds because containers are pre-packaged.
Scalability: Works very well with orchestration tools like Kubernetes for scaling microservices.
Isolation: Each container runs independently, so multiple applications/services can coexist without conflicts.
Portability: I can easily move applications between environments (dev → test → prod, or on-prem → cloud).

Example (how I’d put it in an interview):
"Docker is a containerization tool that helps me package applications with dependencies into portable containers.
 I use it because it ensures consistency across environments, makes deployments faster, 
 reduces resource usage compared to VMs, and integrates well with CI/CD and Kubernetes for scaling."

3. Can you describe a situation where you used Docker to solve a specific problem?

"In one of my projects, we faced issues with inconsistent environments —
 the app worked on one machine but failed on another due to dependency mismatches.
 I solved this by containerizing the application using Docker. I wrote a Dockerfile 
 to standardize dependencies and used Docker Compose to run the app along with its database 
 in a single stack. For development, I used bind mounts for live code changes, and for production, 
 I switched to volumes for persistent data. This ensured consistency across dev, staging, and prod. 
 As a result, we eliminated the ‘works on my machine’ problem and reduced deployment issues significantly."

4. Can you explain how Docker container differs from virtual machines?

Docker Containers
"Docker containers are lightweight, portable units that package an application with its dependencies 
 and run on top of the host operating system’s kernel. They don’t need a full guest OS, which makes 
 them fast to start, efficient in resource usage, and easy to scale. Containers are ideal for microservices, 
 cloud-native applications, and CI/CD pipelines where speed, portability, and consistency across environments are critical."

Virtual Machines (VMs)
"Virtual machines run on a hypervisor and include a full guest operating system along with the 
application and its dependencies. This makes them heavier, more resource-intensive, and slower to 
boot compared to containers. However, VMs provide stronger isolation and allow running multiple,
different operating systems on the same host, which is useful for legacy applications 
or workloads that require complete OS-level separation."

5.You're in charge of maintaining Docker environments in your company and You've noticed many 
stopped containers and unused networks taking up space. 
Describe how you would clean up these resources effectively to optimize the Docker environment.?

The docker prune command is used to clean up unused Docker resources, such as containers, 
volumes, networks, and images. It helps reclaim disk space and tidy up 
the Docker environment by removing objects that are not in use.

There are different types of `docker prune` commands:

- `docker container prune`: Removes stopped containers.
- `docker volume prune`: Deletes unused volumes.
- `docker network prune`: Cleans up unused networks.
- `docker image prune`: Removes unused images.
- docker system prune --volumes
- docker system prune -a : 
remove all in sing cmd
Removes all stopped containers
Removes all unused networks (not used by any container)
Removes all dangling images (images not tagged or referenced)
Removes all unused images (not just dangling ones — that’s what the -a flag does)

Running **`docker system prune`** combines these functionalities into one command, 
ensuring that Docker removes any resources not associated with a running container.
**** On production hosts without double-checking, because you might remove images/containers that are needed later.

6.You're working on a project that requires Docker containers to persistently store data
How do you handle persistent storage in Docker?

"By default, data inside a Docker container is ephemeral, meaning it’s lost when the container 
stops or is removed. To handle persistent storage, I use Docker volumes or bind mounts depending on the use case.
Volumes: These are Docker-managed and stored under /var/lib/docker/volumes/. They are the best choice for production because they are portable, 
managed by Docker, and can be easily backed up and shared between containers.
Example:
docker run -d --name db -v db_data:/var/lib/mysql mysql:latest

Here, db_data is a named volume that persists even if the container is removed.
Bind Mounts: These map a host directory into the container. They are useful for development when I want live code changes to reflect inside the container. Example:
docker run -d -v $(pwd)/app:/usr/src/app myapp
"In production, I prefer volumes because they are more secure, portable, and independent of the host’s 
filesystem structure. For backups or migration, I can also use helper containers to copy data in and out of volumes safely."
cmds:
docker volume create myvolume
docker volume ls
docker volume inspect myvolume
docker volume rm myvolume
docker volume rm $(docker volume ls -q)

Container sees data at → /data
Real data exists on host → /var/lib/docker/volumes/myvolume/_data

7.A company wants to create thousands of Containers. Is there a limit on how many containers you can run in Docker?

"Docker itself does not impose a hard limit on the number of containers you can run. 
The practical limit depends on the host machine’s resources like CPU, memory, disk I/O, 
and network capacity. For example, if the containers are lightweight, you might run thousands on a powerful server, 
but if they are heavy (like full databases), you may only run a few. Scaling beyond a single host is usually managed 
with orchestration tools like Kubernetes or Docker Swarm, which distribute containers across multiple nodes. 
So the real limit is not Docker itself, but the available resources and the orchestration strategy you use."

8.You're managing a Docker environment and need to ensure that each container operates within defined CPU and memory limits. 
How do you limit the CPU and memory usage of a Docker container?

Docker allows you to limit the CPU and memory usage of a container using resource constraints. 
You can set the CPU limit with the --cpu option and the memory limit with the --memory option 
when running the container using the docker run command.
For example, 
->  docker run --cpu 2 --memory 1g nginx 
limits the container to use a maximum of 2 CPU cores and 1GB of memory.

9. What is Dockerfile and how is it used in Docker?

A Dockerfile is a simple text file that contains a set of instructions to build a Docker image. 
Instead of manually creating images, you write a Dockerfile to define how the image should be built, 
including the base image, dependencies, configuration files, and commands. 
Docker then uses this Dockerfile to create an immutable and portable image that can be run as a container anywhere.

 Key Points
FROM → base image
COPY / ADD → copy files into the image
RUN → run commands during build
CMD / ENTRYPOINT → default command when container starts
EXPOSE → open ports
WORKDIR → set working directory

10. Difference Between CMD and ENTRYPOINT in Dockerfile

"CMD and ENTRYPOINT are instructions in a Dockerfile that define commands to run when a container starts. 
CMD sets the default command, which can be overridden by passing a different command at runtime.
ENTRYPOINT sets the main command, and any arguments provided at runtime are appended to it. 
Best practice is to use ENTRYPOINT as the executable and CMD for default arguments."

CMD ["echo", "Hello CMD"]

docker run myimage          # Output: Hello CMD
docker run myimage echo Hi  # Output: Hi

ENTRYPOINT ["echo", "Hello"]

docker run myimage          # Output: Hello
docker run myimage World    # Output: Hello World

11.You've been tasked with ensuring the application can handle increased loads by scaling 
Docker containers horizontally.How do you scale Docker containers horizontally?

To scale Docker containers horizontally, you can use Docker Swarm or a container orchestration tool 
like Kubernetes. Which lets you create and manage multiple docker containers defining the number of 
replicas in declarative way. (Manifests)

12.What is the difference between a Docker container and a Kubernetes pod?

"A Docker container is a single, isolated application running with its dependencies, using the host OS kernel.
 It packages the app and its environment into a lightweight, portable unit. A Kubernetes pod, 
 on the other hand, is the smallest deployable unit in Kubernetes and can contain one or more containers 
 that share the same network namespace, storage volumes, and configuration. 
 Pods provide an abstraction over containers, allowing Kubernetes to manage scaling, networking,
 and lifecycle, whereas Docker containers are managed individually unless orchestrated. Essentially, 
 a pod is a wrapper around containers that enables orchestration features like scaling, service discovery,
 and self-healing."

 13.You're part of a development team deploying a microservices architecture using Docker containers. 
 One of the containers, critical to the system's functionality, has suddenly started failing without 
 clear error messages.How do you debug issues in a failing Docker container?

 13. You're part of a development team deploying a microservices architecture using Docker containers.
One of the containers, critical to the system's functionality, has suddenly started failing without clear error 
messages.How do you debug issues in a failing Docker container?

"When a Docker container starts failing without clear errors, I follow a systematic debugging approach:"
Check container status and logs
docker ps -a                 # Check container status
docker logs <container-id>   # View logs for errors

Inspect container details
docker inspect <container-id>
Checks configuration, environment variables, volume mounts, network settings, and resource limits.

Access the container interactively
docker exec -it <container-id> /bin/bash


Allows manual inspection of the filesystem, processes, and running services.

Verify dependencies
Check if linked containers, databases, or services are running.
Check volume mounts or config files for missing or incorrect data.
Resource constraints

Ensure CPU/memory limits are not causing OOM kills or throttling:

docker stats
Reproduce issues in isolation

Run the container manually with minimal setup to isolate the failing component:

docker run -it myimage /bin/bash

14. Can you describe a situation where you optimized a Dockerfile for faster build times or smaller image size?

"In one project, our Docker image builds were slow and the image size was large 
because we copied unnecessary files and installed all dependencies. To optimize, I:"
Used a smaller base image (python:3.9-slim) to reduce size.
Implemented multi-stage builds to separate build dependencies from runtime, keeping the final image lean.
Combined RUN commands and cleaned up caches to reduce layers.
Used .dockerignore to avoid copying unnecessary files.
Managed services with Docker Compose to efficiently build and run multi-container setups, speeding up development.
"These changes reduced the image size from 1.2 GB to 350 MB and cut build time from 15 minutes to under 5,
 improving our CI/CD pipeline and deployment speed."
Key interview highlights:
Multi-stage builds → smaller runtime image.
Docker Compose → efficient multi-container builds and orchestration.
Combine RUN commands + slim base images → faster builds.
.dockerignore → avoids copying unnecessary files.

15. How do multi-stage builds optimize Docker images?

Multi-stage builds make Docker images smaller and faster.
We build the application in one stage where all build tools exist, then copy only the required files to a final lightweight image.
So we don’t carry unnecessary dependencies into the final image, which improves performance, security, and reduces image size.

ex:
# Build stage
FROM golang:1.20 AS builder
WORKDIR /app
COPY . .
RUN go build -o app

# Final minimal image
FROM alpine:latest
COPY --from=builder /app/app /app
CMD ["/app"]

16. A container can’t reach the internet. What do you do?

First, I check container network configuration and test connectivity by pinging an external site.
Then I verify Docker network settings, DNS configuration, and whether the host machine itself has internet access.
I also check firewall rules or proxy settings that might block traffic.
If needed, I restart the Docker service or recreate the container network.
Step-by-step (you can mention if interviewer asks deeper)

Check internet from container:
docker exec -it <container> ping google.com
If DNS issue:
cat /etc/resolv.conf
or set Google DNS in Docker config.
Check host internet & firewall rules
Restart Docker service:
sudo systemctl restart docker
Recreate Docker network if corrupted:

docker network rm bridge
docker network create bridge

17. What are the risks of running containers as root?

Running containers as root is risky because if the container is compromised, the attacker may gain root access to the host system. 
It increases the chances of privilege-escalation, unauthorized file access, and security breaches. Root containers can also modify 
host files or Docker daemon resources.So we should always run containers with least-privilege users.

18. How to ensure service readiness in Docker Compose?

We ensure service readiness by using healthchecks and depends_on with health condition. 
A healthcheck verifies if a service is actually ready (not just running). Other services 
wait until the dependent service is marked healthy before starting.

version: '3.9'
services:
  db:
    image: mysql:8
    environment:
      MYSQL_ROOT_PASSWORD: root
    healthcheck:
      test: ["CMD", "mysqladmin", "ping", "-h", "localhost"]
      interval: 5s
      retries: 5

  app:
    image: myapp
    depends_on:
      db:
        condition: service_healthy

19. How do you safely clean up unused Docker objects in production?

In production, I never delete everything blindly.
First, I list unused images, containers, volumes, and networks and review them.
Then I remove only unused and stopped resources using safe prune commands.
I also schedule periodic cleanup in maintenance windows to avoid impact on running services.

List unused resources first
docker ps -a
docker images -a
docker volume ls
docker network ls

2. Prune only unused (safe)
docker container prune
docker image prune
docker network prune
docker volume prune

3. Prune everything unused
(still safe — doesn’t affect running containers)
docker system prune
Add volumes also (extra cautious before running):
docker system prune --volumes

20. How does overlay networking work in Swarm?

Overlay networking in Swarm allows containers running on different Swarm nodes to communicate securely as if they are on the same network.
It creates a virtual distributed network over the host machines using VXLAN tunneling.
docker network create -d overlay mynet
docker service create --name web --network mynet nginx

| Feature                      | Meaning                             |
| ---------------------------- | ----------------------------------- |
| Virtual network across nodes | Cross-host container communication  |
| Based on VXLAN               | Encapsulates traffic securely       |
| Service discovery            | Built-in DNS resolves service names |
| Encrypted networks           | Optional encryption between nodes   |
| Used for multi-node services | Web ↔ DB across nodes               |

21. How do you scan images for vulnerabilities?

I scan Docker images using tools like Trivy and Snyk.
I run scans locally and also integrate them into CI/CD to block images with high-risk vulnerabilities.
I regularly update base images and packages to fix CVEs[CVEs are unique IDs]

# Install (Ubuntu/Debian)
sudo apt install trivy
# Scan image
trivy image nginx:latest
# Show only high + critical issues
trivy image --severity HIGH,CRITICAL nginx:latest

docker scout quickview nginx:latest
# Detailed vulnerability report
docker scout cves nginx:latest

nginx:latest (debian 11)

Total: 4 (HIGH: 3, CRITICAL: 1)

┌────────────┬───────────┬───────────────┬─────────────────────────────────┐
│ Package    │ Severity  │ CVE           │ Title                           │
├────────────┼───────────┼───────────────┼─────────────────────────────────┤
│ openssl    │ CRITICAL  │ CVE-2023-0286 │ Buffer overflow in X.509        │
│ libc6      │ HIGH      │ CVE-2022-37454│ Integer overflow                │
│ curl       │ HIGH      │ CVE-2023-23914│ TLS certificate bypass          │
│ tar        │ HIGH      │ CVE-2021-20193│ Symlink attack issue            │
└────────────┴───────────┴───────────────┴─────────────────────────────────┘


22. How do you manage secrets in Docker?

Docker Secrets is the feature that is exclusive to Docker Swarm.
It allows secure storage sensitive data (like passwords, tokens, keys) into containers.
Secrets are encrypted, not stored in images, and not visible in environment variables.

Example (Docker Swarm only)
echo "mypassword" | docker secret create db_pass -
docker service create --name myapp --secret db_pass nginx
Inside container, the secret appears as a secure file:
/run/secrets/db_pass

23.  A container keeps restarting. What’s the root cause?

A container usually restarts because the main process inside it is crashing or exiting unexpectedly.
Common causes include:

Application error or crash (unhandled exception, service not starting)
Wrong configuration or missing environment variables
Dependency service not ready (DB not reachable)
Failing health check / liveness probe
Incorrect entrypoint or command
Resource issues (low memory → OOMKill)
To debug, I check:
docker ps
docker logs <container>
docker inspect <container> | grep -i restart
If needed:
docker exec -it <container> sh

24. How do you implement canary deployments with Docker and Kubernetes?

Canary deployments with Docker and Kubernetes can be implemented by following these steps:

- Set up a Kubernetes cluster and deploy your application using Docker containers.
- Create a new deployment or service for the updated version of your application (the Canary version).
- Configure the canary deployment to receive a small percentage of traffic while the majority of traffic is still routed to the stable version.
- Monitor the canary deployment, collecting metrics and user feedback to assess its stability and performance.
- If the canary deployment proves successful, gradually increase its traffic allocation.
- If any issues arise, quickly roll back by reducing the canary's traffic allocation or rolling back the deployment.
- Repeat the process, gradually increasing the canary's traffic allocation until it becomes the new stable version.
