{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0910a503",
   "metadata": {},
   "source": [
    "1. How do you debug a pod stuck in CrashLoopBackoff?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec0f3e4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "--> “If a pod is stuck in CrashLoopBackOff, I usually follow a structured debugging process. First, I check the pod’s status with kubectl get pods to confirm the issue.\n",
    "--> Then I run kubectl describe pod <pod-name> to see events, like whether it’s failing due to liveness/readiness probes, OOMKilled, or some other scheduling issue.\n",
    "--> Next, I check the container logs using kubectl logs <pod-name>, and if the pod is restarting too quickly, I use the --previous flag to capture logs from the previous crash. [kubectl logs <pod-name> --previous\n",
    "] if pod as multiple container [kubectl logs <pod-name> -c <container-name> --previous]\n",
    "Based on experience, common causes are application errors, misconfigured environment variables, wrong commands/entrypoints in the image, failed probes, or resource constraints. For example, if the logs show ‘OOMKilled’, I would increase memory limits. If the liveness probe is failing, I would adjust the probe configuration or fix the app’s startup.\n",
    "If logs don’t help, I use kubectl debug or override the entrypoint with a sleep command so I can exec into the container and inspect configs or dependencies.\n",
    "So my approach is: check describe, check logs, validate probes and resource limits, and then fix the underlying application or configuration issue before redeploying.”*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf2d9fa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "2.How will you stop pod in k8s?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23435d93",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "--> *“In Kubernetes, we normally don’t stop a pod directly, because pods are managed by higher-level controllers like Deployments, ReplicaSets, or StatefulSets. If I just delete the pod using kubectl delete pod <pod-name>, the controller will immediately recreate it to maintain the desired replicas.\n",
    "If I really want to stop the pod permanently, \n",
    "\n",
    "--> I need to scale down the controller (for example, kubectl scale deployment <name> --replicas=0). That way, Kubernetes won’t recreate the pod.\n",
    "cmd: kubectl scale deployment sindhu-deployment  --replicas=0 -n default \n",
    "\n",
    "--> If it’s a standalone pod without a controller, deleting it with kubectl delete pod will stop it.\n",
    "\n",
    "--> For debugging, I can also temporarily ‘stop’ a pod by running kubectl drain on the node, or by setting replicas=0 in the YAML manifest.”*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1787bba",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "3. how will you replicate a pod ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f25654",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "--> *“In Kubernetes, we don’t usually replicate a single pod manually because pods are ephemeral and managed by controllers. Instead, we use controllers like Deployments, ReplicaSets, or StatefulSets to handle replication automatically.\n",
    "    For example, if I have a pod defined in a Deployment, I can increase the number of replicas by updating the replicas field:*\n",
    "\n",
    "--> kubectl scale deployment <deployment-name> --replicas=3\n",
    "\n",
    "--> *This ensures Kubernetes maintains 3 copies of the pod. If a pod crashes, the controller automatically recreates it.\n",
    "\n",
    "--> For standalone pods (not managed by a controller), I would need to manually create additional pods with the same specification, but that’s not recommended in production.”*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a47317",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "4. command to get logs in k8s?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae37918",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "“To check logs in Kubernetes, I use kubectl logs <pod-name>.\n",
    " If the pod has multiple containers, I specify the container with -c <container-name>. \n",
    " For pods that are crashing repeatedly, I use the --previous flag to see logs from the last crashed instance.\n",
    " And if the pod is in a different namespace, I add -n <namespace>.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d165f21",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "5.what will you do if pod is getting more load and we need to stay it healthy before it gets died?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5a9659",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "“If a pod is experiencing high load and we want to keep it healthy, I would handle it in multiple ways:\n",
    "\n",
    "Horizontal Scaling: Increase the number of pod replicas using a Deployment or ReplicaSet. For example:*\n",
    "\n",
    "kubectl scale deployment <deployment-name> --replicas=3\n",
    "\n",
    "Or set up a Horizontal Pod Autoscaler (HPA) to automatically scale based on CPU/memory load:\n",
    "\n",
    "kubectl autoscale deployment <deployment-name> --cpu-percent=50 --min=2 --max=10\n",
    "\n",
    "Vertical Scaling: Increase the resources (CPU/memory) allocated to the pod in its YAML spec to handle higher load.\n",
    "\n",
    "Load Distribution: Ensure the service is properly load-balanced across the pods.\n",
    "\n",
    "Readiness/Liveness Probes: Check that probes are configured correctly so Kubernetes can restart unhealthy pods before they fail completely.\n",
    "\n",
    "So my approach is proactive: scale horizontally, ensure sufficient resources, and configure proper probes to maintain pod health under high load.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b43924a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "6. How do you manage multiple environments (dev, stage, prod)?\n",
    "\n",
    "Separate namespaces or clusters.\n",
    "\n",
    "Use Helm or Kustomize for environment-specific configs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480ac3b6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\"Auto-generated Terraform code is useful to bootstrap,\n",
    " but it’s not production-ready. It usually contains hardcoded values,\n",
    "  lacks variables and modules, and doesn’t follow best practices like state management or secrets handling.\n",
    "   We should refactor it into reusable modules with variables, \n",
    "  outputs, and remote backends before using it for real infrastructure.\""
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
